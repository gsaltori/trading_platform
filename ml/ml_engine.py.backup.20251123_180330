# ml/ml_engine.py
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
import xgboost as xgb
import lightgbm as lgb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import joblib
import warnings
import logging
from typing import Dict, List, Optional, Tuple, Any, Callable, Union
from dataclasses import dataclass, field
import talib as ta
from ta import add_all_ta_features
from ta.momentum import RSIIndicator, StochasticOscillator
from ta.trend import MACD, EMAIndicator, ADXIndicator
from ta.volatility import BollingerBands, AverageTrueRange
from typing import Dict, List, Optional, Tuple, Any, Callable, Union

logger = logging.getLogger(__name__)

@dataclass
class MLModelConfig:
    model_type: str  # 'classification', 'regression', 'clustering'
    algorithm: str   # 'random_forest', 'xgboost', 'lstm', 'ensemble'
    features: List[str]
    target: str
    lookback_window: int = 50
    prediction_horizon: int = 1
    train_test_split: float = 0.8
    parameters: Dict[str, Any] = field(default_factory=dict)

@dataclass
class MLResult:
    model_name: str
    predictions: np.ndarray
    probabilities: Optional[np.ndarray]
    actuals: np.ndarray
    metrics: Dict[str, float]
    model: Any
    feature_importance: Optional[pd.DataFrame] = None
    classification_report: Optional[Dict] = None

class FeatureEngineer:
    """Motor de ingeniería de características avanzado"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.feature_selector = None
        self.fitted = False
        
    def create_technical_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """Crear características técnicas avanzadas"""
        df = data.copy()
        
        # Precio y retornos
        df['returns'] = df['close'].pct_change()
        df['log_returns'] = np.log(df['close'] / df['close'].shift(1))
        df['price_range'] = df['high'] - df['low']
        df['normalized_range'] = df['price_range'] / df['close']
        
        # Medias móviles y sus cruces
        for window in [5, 10, 20, 50, 100]:
            df[f'sma_{window}'] = ta.SMA(df['close'], timeperiod=window)
            df[f'ema_{window}'] = ta.EMA(df['close'], timeperiod=window)
            df[f'sma_ratio_{window}'] = df['close'] / df[f'sma_{window}'] - 1
            df[f'ema_ratio_{window}'] = df['close'] / df[f'ema_{window}'] - 1
        
        # RSI múltiples timeframes
        for period in [7, 14, 21]:
            df[f'rsi_{period}'] = RSIIndicator(df['close'], window=period).rsi()
        
        # MACD
        macd = MACD(df['close'])
        df['macd'] = macd.macd()
        df['macd_signal'] = macd.macd_signal()
        df['macd_histogram'] = macd.macd_diff()
        
        # Bollinger Bands
        bb = BollingerBands(df['close'])
        df['bb_upper'] = bb.bollinger_hband()
        df['bb_lower'] = bb.bollinger_lband()
        df['bb_middle'] = bb.bollinger_mavg()
        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']
        df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
        
        # Estocástico
        stoch = StochasticOscillator(df['high'], df['low'], df['close'])
        df['stoch_k'] = stoch.stoch()
        df['stoch_d'] = stoch.stoch_signal()
        
        # ADX y direccional
        adx = ADXIndicator(df['high'], df['low'], df['close'])
        df['adx'] = adx.adx()
        df['di_plus'] = adx.adx_pos()
        df['di_minus'] = adx.adx_neg()
        
        # ATR
        atr = AverageTrueRange(df['high'], df['low'], df['close'])
        df['atr'] = atr.average_true_range()
        df['atr_pct'] = df['atr'] / df['close']
        
        # Volumen
        df['volume_sma'] = ta.SMA(df['volume'], timeperiod=20)
        df['volume_ratio'] = df['volume'] / df['volume_sma']
        df['volume_price_trend'] = df['volume'] * df['returns']
        
        # Volatilidad
        for window in [10, 20, 30]:
            df[f'volatility_{window}'] = df['returns'].rolling(window).std()
        
        # Momentum
        for period in [5, 10, 20]:
            df[f'momentum_{period}'] = df['close'] / df['close'].shift(period) - 1
        
        # Patrones de precio
        df['higher_high'] = (df['high'] > df['high'].shift(1)).astype(int)
        df['lower_low'] = (df['low'] < df['low'].shift(1)).astype(int)
        df['inside_bar'] = ((df['high'] < df['high'].shift(1)) & 
                           (df['low'] > df['low'].shift(1))).astype(int)
        
        # Características de tendencia
        df['trend_strength'] = df['adx'] / 100.0
        df['trend_direction'] = np.where(df['di_plus'] > df['di_minus'], 1, -1)
        
        return df
    
    def create_lagged_features(self, data: pd.DataFrame, columns: List[str], 
                             lags: List[int]) -> pd.DataFrame:
        """Crear características retrasadas"""
        df = data.copy()
        for col in columns:
            for lag in lags:
                df[f'{col}_lag_{lag}'] = df[col].shift(lag)
        return df
    
    def create_rolling_features(self, data: pd.DataFrame, columns: List[str], 
                              windows: List[int]) -> pd.DataFrame:
        """Crear características rolling"""
        df = data.copy()
        for col in columns:
            for window in windows:
                df[f'{col}_roll_mean_{window}'] = df[col].rolling(window).mean()
                df[f'{col}_roll_std_{window}'] = df[col].rolling(window).std()
                df[f'{col}_roll_min_{window}'] = df[col].rolling(window).min()
                df[f'{col}_roll_max_{window}'] = df[col].rolling(window).max()
                df[f'{col}_roll_skew_{window}'] = df[col].rolling(window).skew()
                df[f'{col}_roll_kurt_{window}'] = df[col].rolling(window).kurt()
        return df
    
    def create_target_variable(self, data: pd.DataFrame, horizon: int = 1, 
                             method: str = 'classification') -> pd.Series:
        """Crear variable target para ML"""
        if method == 'classification':
            # Clasificación: 1 si precio sube, 0 si baja
            future_returns = data['close'].shift(-horizon) / data['close'] - 1
            target = (future_returns > 0).astype(int)
        elif method == 'regression':
            # Regresión: retorno futuro
            target = data['close'].shift(-horizon) / data['close'] - 1
        else:
            raise ValueError("Método debe ser 'classification' o 'regression'")
        
        return target
    
    # ml/ml_engine.py - FIXED prepare_features method

    def prepare_features(self, data: pd.DataFrame, target: pd.Series, 
                        fit: bool = True) -> Tuple[pd.DataFrame, pd.Series]:
        """Preparar características para ML"""
        
        # Validar datos de entrada
        if len(data) < 100:
            raise ValueError(f"Insufficient data: {len(data)} rows. Need at least 100 rows.")
        
        # Crear características técnicas
        df = self.create_technical_features(data)
        
        # Crear características retrasadas (REDUCIDO para evitar demasiados NaN)
        price_columns = ['open', 'high', 'low', 'close', 'volume']
        existing_price_columns = [col for col in price_columns if col in df.columns]
        df = self.create_lagged_features(df, existing_price_columns, [1, 2, 3])  # Solo 3 lags
        
        # Crear características rolling (REDUCIDO)
        rolling_columns = []
        if 'returns' in df.columns:
            rolling_columns.append('returns')
        if 'volume' in df.columns:
            rolling_columns.append('volume')
        if 'atr' in df.columns:
            rolling_columns.append('atr')
        
        if rolling_columns:
            df = self.create_rolling_features(df, rolling_columns, [5, 10])  # Solo 2 ventanas
        
        # Alinear con target y eliminar NaN
        aligned_data = pd.concat([df, target], axis=1).dropna()
        
        # Verificar que tenemos suficientes datos
        if len(aligned_data) < 10:
            raise ValueError(f"After removing NaN, only {len(aligned_data)} samples remain. "
                            f"Original data had {len(data)} rows. Need at least 10 samples.")
        
        X = aligned_data.iloc[:, :-1]
        y = aligned_data.iloc[:, -1]
        
        # Limpiar datos
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.dropna(axis=1, how='all')
        X = X.loc[:, X.std() > 0]
        
        if X.shape[1] == 0:
            raise ValueError("No valid features after preprocessing")
        
        # Escalar características
        if fit:
            X_scaled = self.scaler.fit_transform(X)
            self.fitted = True
        else:
            if not self.fitted:
                raise ValueError("Scaler not fitted. Call with fit=True first.")
            X_scaled = self.scaler.transform(X)
        
        return pd.DataFrame(X_scaled, columns=X.columns, index=X.index), y

    def __init__(self):
        self.scaler = StandardScaler()
        self.feature_selector = None
        self.fitted = False
        self.models = {}
        self.results = {}
        
    def train_model(self, data: pd.DataFrame, config: MLModelConfig) -> MLResult:
        """Entrenar modelo de ML"""
        logger.info(f"Entrenando modelo {config.algorithm} para {config.target}")
        
        try:
            # Crear target
            target = self.feature_engineer.create_target_variable(
                data, config.prediction_horizon, config.model_type
            )
            
            # Preparar características
            X, y = self.feature_engineer.prepare_features(data, target, fit=True)
            
            # Split temporal (no shuffle para series de tiempo)
            split_idx = int(len(X) * config.train_test_split)
            X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
            y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
            
            # Entrenar modelo
            model = self._create_model(config)
            
            if config.algorithm == 'lstm':
                # Preparar datos para LSTM
                X_train_3d = self._reshape_for_lstm(X_train.values, config.lookback_window)
                X_test_3d = self._reshape_for_lstm(X_test.values, config.lookback_window)
                
                # Ajustar y_train para LSTM
                y_train_lstm = y_train.iloc[config.lookback_window:]
                y_test_lstm = y_test.iloc[config.lookback_window:]
                
                # Entrenar LSTM
                history = model.fit(
                    X_train_3d, y_train_lstm,
                    validation_data=(X_test_3d, y_test_lstm),
                    epochs=config.parameters.get('epochs', 50),
                    batch_size=config.parameters.get('batch_size', 32),
                    verbose=0,
                    callbacks=[
                        EarlyStopping(patience=10, restore_best_weights=True),
                        ReduceLROnPlateau(patience=5, factor=0.5)
                    ]
                )
                
                # Predecir
                predictions = model.predict(X_test_3d)
                probabilities = predictions if config.model_type == 'classification' else None
                
            else:
                # Modelos tradicionales
                model.fit(X_train, y_train)
                
                # Predecir
                if hasattr(model, 'predict_proba') and config.model_type == 'classification':
                    probabilities = model.predict_proba(X_test)
                    predictions = model.predict(X_test)
                else:
                    predictions = model.predict(X_test)
                    probabilities = None
            
            # Calcular métricas
            metrics = self._calculate_metrics(y_test, predictions, probabilities, config.model_type)
            
            # Importancia de características
            feature_importance = self._get_feature_importance(model, X_train, config.algorithm)
            
            # Reporte de clasificación
            classification_report_dict = None
            if config.model_type == 'classification':
                classification_report_dict = classification_report(
                    y_test, predictions, output_dict=True
                )
            
            # Guardar resultados
            result = MLResult(
                model_name=f"{config.algorithm}_{config.target}",
                predictions=predictions,
                probabilities=probabilities,
                actuals=y_test.values,
                metrics=metrics,
                model=model,
                feature_importance=feature_importance,
                classification_report=classification_report_dict
            )
            
            self.models[result.model_name] = model
            self.results[result.model_name] = result
            
            logger.info(f"Modelo {result.model_name} entrenado. Accuracy: {metrics.get('accuracy', 0):.3f}")
            return result
            
        except Exception as e:
            logger.error(f"Error entrenando modelo: {e}")
            raise
    
    def _create_model(self, config: MLModelConfig) -> Any:
        """Crear modelo según configuración"""
        params = config.parameters
        
        if config.algorithm == 'random_forest':
            return RandomForestClassifier(
                n_estimators=params.get('n_estimators', 100),
                max_depth=params.get('max_depth', 10),
                random_state=params.get('random_state', 42)
            )
            
        elif config.algorithm == 'xgboost':
            return xgb.XGBClassifier(
                n_estimators=params.get('n_estimators', 100),
                max_depth=params.get('max_depth', 6),
                learning_rate=params.get('learning_rate', 0.1),
                random_state=params.get('random_state', 42)
            )
            
        elif config.algorithm == 'lightgbm':
            return lgb.LGBMClassifier(
                n_estimators=params.get('n_estimators', 100),
                max_depth=params.get('max_depth', -1),
                learning_rate=params.get('learning_rate', 0.1),
                random_state=params.get('random_state', 42)
            )
            
        elif config.algorithm == 'svm':
            return SVC(
                C=params.get('C', 1.0),
                kernel=params.get('kernel', 'rbf'),
                probability=True,
                random_state=params.get('random_state', 42)
            )
            
        elif config.algorithm == 'mlp':
            return MLPClassifier(
                hidden_layer_sizes=params.get('hidden_layer_sizes', (100, 50)),
                activation=params.get('activation', 'relu'),
                learning_rate_init=params.get('learning_rate', 0.001),
                random_state=params.get('random_state', 42),
                max_iter=params.get('max_iter', 1000)
            )
            
        elif config.algorithm == 'lstm':
            model = Sequential([
                LSTM(units=params.get('lstm_units', 50), 
                     return_sequences=True, 
                     input_shape=(config.lookback_window, len(config.features))),
                Dropout(params.get('dropout_rate', 0.2)),
                LSTM(units=params.get('lstm_units', 50), return_sequences=False),
                Dropout(params.get('dropout_rate', 0.2)),
                Dense(units=params.get('dense_units', 25), activation='relu'),
                Dense(1, activation='sigmoid' if config.model_type == 'classification' else 'linear')
            ])
            
            optimizer = Adam(learning_rate=params.get('learning_rate', 0.001))
            loss = 'binary_crossentropy' if config.model_type == 'classification' else 'mse'
            
            model.compile(optimizer=optimizer, loss=loss, 
                         metrics=['accuracy'] if config.model_type == 'classification' else ['mae'])
            return model
            
        elif config.algorithm == 'ensemble':
            # Ensemble de múltiples modelos
            estimators = []
            
            # Random Forest
            estimators.append(('rf', RandomForestClassifier(
                n_estimators=100, random_state=42
            )))
            
            # XGBoost
            estimators.append(('xgb', xgb.XGBClassifier(
                n_estimators=100, random_state=42
            )))
            
            return VotingClassifier(estimators=estimators, voting='soft')
        
        else:
            raise ValueError(f"Algoritmo no soportado: {config.algorithm}")
    
    def _reshape_for_lstm(self, X: np.ndarray, lookback: int) -> np.ndarray:
        """Reformatear datos para LSTM"""
        X_3d = []
        for i in range(lookback, len(X)):
            X_3d.append(X[i-lookback:i, :])
        return np.array(X_3d)
    
    def _calculate_metrics(self, y_true: pd.Series, y_pred: np.ndarray, 
                          probabilities: Optional[np.ndarray], model_type: str) -> Dict[str, float]:
        """Calcular métricas de evaluación"""
        if model_type == 'classification':
            metrics = {
                'accuracy': accuracy_score(y_true, y_pred),
                'precision': precision_score(y_true, y_pred, average='weighted'),
                'recall': recall_score(y_true, y_pred, average='weighted'),
                'f1': precision_score(y_true, y_pred, average='weighted')  # Simplificado
            }
            
            if probabilities is not None:
                metrics['log_loss'] = -np.mean(
                    y_true * np.log(probabilities[:, 1] + 1e-15) + 
                    (1 - y_true) * np.log(1 - probabilities[:, 1] + 1e-15)
                )
                
        else:  # regression
            errors = y_true - y_pred
            metrics = {
                'mse': np.mean(errors ** 2),
                'rmse': np.sqrt(np.mean(errors ** 2)),
                'mae': np.mean(np.abs(errors)),
                'r2': 1 - (np.sum(errors ** 2) / np.sum((y_true - np.mean(y_true)) ** 2))
            }
        
        return metrics
    
    def _get_feature_importance(self, model: Any, X: pd.DataFrame, 
                              algorithm: str) -> Optional[pd.DataFrame]:
        """Obtener importancia de características"""
        try:
            if algorithm in ['random_forest', 'xgboost', 'lightgbm']:
                if hasattr(model, 'feature_importances_'):
                    importance_df = pd.DataFrame({
                        'feature': X.columns,
                        'importance': model.feature_importances_
                    }).sort_values('importance', ascending=False)
                    return importance_df
            return None
        except Exception as e:
            logger.warning(f"No se pudo obtener importancia de características: {e}")
            return None
    
    def predict(self, model_name: str, data: pd.DataFrame) -> np.ndarray:
        """Realizar predicciones con modelo entrenado"""
        if model_name not in self.models:
            raise ValueError(f"Modelo {model_name} no encontrado")
        
        model = self.models[model_name]
        config = self.results[model_name].model_config  # Necesitaríamos guardar la config
        
        # Crear target dummy para preparar características
        dummy_target = pd.Series(index=data.index, data=0)
        X_prepared, _ = self.feature_engineer.prepare_features(data, dummy_target, fit=False)
        
        if hasattr(model, 'predict'):
            return model.predict(X_prepared)
        else:
            raise ValueError("Modelo no tiene método predict")
    
    def save_model(self, model_name: str, filepath: str):
        """Guardar modelo en disco"""
        if model_name not in self.models:
            raise ValueError(f"Modelo {model_name} no encontrado")
        
        joblib.dump({
            'model': self.models[model_name],
            'feature_engineer': self.feature_engineer,
            'result': self.results[model_name]
        }, filepath)
        
        logger.info(f"Modelo {model_name} guardado en {filepath}")
    
    def load_model(self, model_name: str, filepath: str):
        """Cargar modelo desde disco"""
        saved_data = joblib.load(filepath)
        
        self.models[model_name] = saved_data['model']
        self.feature_engineer = saved_data['feature_engineer']
        self.results[model_name] = saved_data['result']
        
        logger.info(f"Modelo {model_name} cargado desde {filepath}")


class MLEngine:
    """Motor principal de Machine Learning"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.feature_selector = None
        self.fitted = False
        self.models = {}
        self.results = {}
        
    def train_model(self, data: pd.DataFrame, config: MLModelConfig) -> MLResult:
        """Entrenar modelo de ML"""
        logger.info(f"Entrenando modelo {config.algorithm} para {config.target}")
        
        try:
            # Crear target
            target = self.feature_engineer.create_target_variable(
                data, config.prediction_horizon, config.model_type
            )
            
            # Preparar características
            X, y = self.feature_engineer.prepare_features(data, target, fit=True)
            
            # Split temporal (no shuffle para series de tiempo)
            split_idx = int(len(X) * config.train_test_split)
            X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
            y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
            
            # Entrenar modelo
            model = self._create_model(config)
            
            if config.algorithm == 'lstm':
                # Preparar datos para LSTM
                X_train_3d = self._reshape_for_lstm(X_train.values, config.lookback_window)
                X_test_3d = self._reshape_for_lstm(X_test.values, config.lookback_window)
                
                # Ajustar y_train para LSTM
                y_train_lstm = y_train.iloc[config.lookback_window:]
                y_test_lstm = y_test.iloc[config.lookback_window:]
                
                # Entrenar LSTM
                history = model.fit(
                    X_train_3d, y_train_lstm,
                    validation_data=(X_test_3d, y_test_lstm),
                    epochs=config.parameters.get('epochs', 50),
                    batch_size=config.parameters.get('batch_size', 32),
                    verbose=0,
                    callbacks=[
                        EarlyStopping(patience=10, restore_best_weights=True),
                        ReduceLROnPlateau(patience=5, factor=0.5)
                    ]
                )
                
                # Predecir
                predictions = model.predict(X_test_3d)
                probabilities = predictions if config.model_type == 'classification' else None
                
            else:
                # Modelos tradicionales
                model.fit(X_train, y_train)
                
                # Predecir
                if hasattr(model, 'predict_proba') and config.model_type == 'classification':
                    probabilities = model.predict_proba(X_test)
                    predictions = model.predict(X_test)
                else:
                    predictions = model.predict(X_test)
                    probabilities = None
            
            # Calcular métricas
            metrics = self._calculate_metrics(y_test, predictions, probabilities, config.model_type)
            
            # Importancia de características
            feature_importance = self._get_feature_importance(model, X_train, config.algorithm)
            
            # Reporte de clasificación
            classification_report_dict = None
            if config.model_type == 'classification':
                from sklearn.metrics import classification_report
                classification_report_dict = classification_report(
                    y_test, predictions, output_dict=True
                )
            
            # Guardar resultados
            result = MLResult(
                model_name=f"{config.algorithm}_{config.target}",
                predictions=predictions,
                probabilities=probabilities,
                actuals=y_test.values,
                metrics=metrics,
                model=model,
                feature_importance=feature_importance,
                classification_report=classification_report_dict
            )
            
            self.models[result.model_name] = model
            self.results[result.model_name] = result
            
            logger.info(f"Modelo {result.model_name} entrenado. Accuracy: {metrics.get('accuracy', 0):.3f}")
            return result
            
        except Exception as e:
            logger.error(f"Error entrenando modelo: {e}")
            raise
    
    def _create_model(self, config: MLModelConfig):
        """Crear modelo según configuración"""
        params = config.parameters
        
        if config.algorithm == 'random_forest':
            return RandomForestClassifier(
                n_estimators=params.get('n_estimators', 100),
                max_depth=params.get('max_depth', 10),
                random_state=params.get('random_state', 42)
            )
            
        elif config.algorithm == 'xgboost':
            return xgb.XGBClassifier(
                n_estimators=params.get('n_estimators', 100),
                max_depth=params.get('max_depth', 6),
                learning_rate=params.get('learning_rate', 0.1),
                random_state=params.get('random_state', 42)
            )
            
        elif config.algorithm == 'lightgbm':
            return lgb.LGBMClassifier(
                n_estimators=params.get('n_estimators', 100),
                max_depth=params.get('max_depth', -1),
                learning_rate=params.get('learning_rate', 0.1),
                random_state=params.get('random_state', 42)
            )
            
        elif config.algorithm == 'svm':
            return SVC(
                C=params.get('C', 1.0),
                kernel=params.get('kernel', 'rbf'),
                probability=True,
                random_state=params.get('random_state', 42)
            )
            
        elif config.algorithm == 'mlp':
            return MLPClassifier(
                hidden_layer_sizes=params.get('hidden_layer_sizes', (100, 50)),
                activation=params.get('activation', 'relu'),
                learning_rate_init=params.get('learning_rate', 0.001),
                random_state=params.get('random_state', 42),
                max_iter=params.get('max_iter', 1000)
            )
            
        elif config.algorithm == 'lstm':
            model = Sequential([
                LSTM(units=params.get('lstm_units', 50), 
                     return_sequences=True, 
                     input_shape=(config.lookback_window, len(config.features) if config.features else 10)),
                Dropout(params.get('dropout_rate', 0.2)),
                LSTM(units=params.get('lstm_units', 50), return_sequences=False),
                Dropout(params.get('dropout_rate', 0.2)),
                Dense(units=params.get('dense_units', 25), activation='relu'),
                Dense(1, activation='sigmoid' if config.model_type == 'classification' else 'linear')
            ])
            
            optimizer = Adam(learning_rate=params.get('learning_rate', 0.001))
            loss = 'binary_crossentropy' if config.model_type == 'classification' else 'mse'
            
            model.compile(optimizer=optimizer, loss=loss, 
                         metrics=['accuracy'] if config.model_type == 'classification' else ['mae'])
            return model
            
        elif config.algorithm == 'ensemble':
            estimators = []
            estimators.append(('rf', RandomForestClassifier(n_estimators=100, random_state=42)))
            estimators.append(('xgb', xgb.XGBClassifier(n_estimators=100, random_state=42)))
            return VotingClassifier(estimators=estimators, voting='soft')
        
        else:
            raise ValueError(f"Algoritmo no soportado: {config.algorithm}")
    
    def _reshape_for_lstm(self, X: np.ndarray, lookback: int) -> np.ndarray:
        """Reformatear datos para LSTM"""
        X_3d = []
        for i in range(lookback, len(X)):
            X_3d.append(X[i-lookback:i, :])
        return np.array(X_3d)
    
    def _calculate_metrics(self, y_true: pd.Series, y_pred: np.ndarray, 
                          probabilities: Optional[np.ndarray], model_type: str) -> Dict[str, float]:
        """Calcular métricas de evaluación"""
        if model_type == 'classification':
            metrics = {
                'accuracy': accuracy_score(y_true, y_pred),
                'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),
                'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),
                'f1': precision_score(y_true, y_pred, average='weighted', zero_division=0)
            }
            
            if probabilities is not None and len(probabilities.shape) > 1:
                metrics['log_loss'] = -np.mean(
                    y_true * np.log(probabilities[:, 1] + 1e-15) + 
                    (1 - y_true) * np.log(1 - probabilities[:, 1] + 1e-15)
                )
                
        else:  # regression
            errors = y_true - y_pred
            metrics = {
                'mse': np.mean(errors ** 2),
                'rmse': np.sqrt(np.mean(errors ** 2)),
                'mae': np.mean(np.abs(errors)),
                'r2': 1 - (np.sum(errors ** 2) / np.sum((y_true - np.mean(y_true)) ** 2))
            }
        
        return metrics
    
    def _get_feature_importance(self, model, X: pd.DataFrame, 
                              algorithm: str) -> Optional[pd.DataFrame]:
        """Obtener importancia de características"""
        try:
            if algorithm in ['random_forest', 'xgboost', 'lightgbm']:
                if hasattr(model, 'feature_importances_'):
                    importance_df = pd.DataFrame({
                        'feature': X.columns,
                        'importance': model.feature_importances_
                    }).sort_values('importance', ascending=False)
                    return importance_df
            return None
        except Exception as e:
            logger.warning(f"No se pudo obtener importancia de características: {e}")
            return None
    
    def predict(self, model_name: str, data: pd.DataFrame) -> np.ndarray:
        """Realizar predicciones con modelo entrenado"""
        if model_name not in self.models:
            raise ValueError(f"Modelo {model_name} no encontrado")
        
        model = self.models[model_name]
        dummy_target = pd.Series(index=data.index, data=0)
        X_prepared, _ = self.feature_engineer.prepare_features(data, dummy_target, fit=False)
        
        if hasattr(model, 'predict'):
            return model.predict(X_prepared)
        else:
            raise ValueError("Modelo no tiene método predict")
    
    def save_model(self, model_name: str, filepath: str):
        """Guardar modelo en disco"""
        if model_name not in self.models:
            raise ValueError(f"Modelo {model_name} no encontrado")
        
        joblib.dump({
            'model': self.models[model_name],
            'feature_engineer': self.feature_engineer,
            'result': self.results[model_name]
        }, filepath)
        
        logger.info(f"Modelo {model_name} guardado en {filepath}")
    
    def load_model(self, model_name: str, filepath: str):
        """Cargar modelo desde disco"""
        saved_data = joblib.load(filepath)
        
        self.models[model_name] = saved_data['model']
        self.feature_engineer = saved_data['feature_engineer']
        self.results[model_name] = saved_data['result']
        
        logger.info(f"Modelo {model_name} cargado desde {filepath}")


class MarketRegimeDetector:
    """Detector de regímenes de mercado usando ML"""
    
    def __init__(self):
        self.ml_engine = MLEngine()
        self.regime_model = None
        
    def detect_regimes(self, data: pd.DataFrame, n_regimes: int = 3) -> pd.Series:
        """Detectar regímenes de mercado usando clustering"""
        from sklearn.cluster import KMeans
        from sklearn.preprocessing import StandardScaler
        
        # Crear características para detección de regímenes
        features = pd.DataFrame()
        
        # Volatilidad
        features['volatility'] = data['close'].pct_change().rolling(20).std()
        
        # Tendencia
        features['trend'] = data['close'] / data['close'].rolling(50).mean() - 1
        
        # Rango de trading
        features['range'] = (data['high'] - data['low']) / data['close']
        
        # Volumen
        features['volume_z'] = (data['volume'] - data['volume'].rolling(20).mean()) / data['volume'].rolling(20).std()
        
        features = features.dropna()
        
        # Escalar características
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)
        
        # Clustering
        kmeans = KMeans(n_clusters=n_regimes, random_state=42)
        regimes = kmeans.fit_predict(features_scaled)
        
        # Mapear clusters a regímenes interpretables
        regime_map = self._interpret_regimes(features, regimes)
        
        return pd.Series(regime_map, index=features.index)
    
    def _interpret_regimes(self, features: pd.DataFrame, regimes: np.ndarray) -> List[str]:
        """Interpretar clusters como regímenes de mercado"""
        regime_descriptions = []
        
        for i in range(len(np.unique(regimes))):
            cluster_data = features[regimes == i]
            
            avg_volatility = cluster_data['volatility'].mean()
            avg_trend = cluster_data['trend'].mean()
            avg_range = cluster_data['range'].mean()
            
            if avg_volatility > features['volatility'].quantile(0.7):
                if abs(avg_trend) > 0.02:
                    regime = "Tendencial Volátil"
                else:
                    regime = "Lateral Volátil"
            else:
                if abs(avg_trend) > 0.01:
                    regime = "Tendencial Tranquilo"
                else:
                    regime = "Lateral Tranquilo"
            
            regime_descriptions.append(regime)
        
        # Mapear cada cluster a su descripción
        return [regime_descriptions[regime] for regime in regimes]